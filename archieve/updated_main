#Chat history

from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.llms import Ollama
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from gtesmall2 import gtesmall

# Set the correct path to the "PDFs" folder in your Google Drive
pdfs_folder_path = '/home/navin/rag/PDFs'

# Load model directly
llm = Ollama(model="phi", callbacks=[StreamingStdOutCallbackHandler()])

# Initialize the directory loader with the correct path
loader = DirectoryLoader('PDFs', use_multithreading=True, silent_errors=True,loader_cls=PyPDFLoader).load()

# Verify that raw_documents is not empty
assert loader, "No documents found in the specified folder."

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = text_splitter.split_documents(loader)

# Verify that documents is not empty
assert documents, "No documents were split into chunks."

# Load the embeddings into Chroma
print("Loading documents into Chroma\n")
db = Chroma.from_documents(documents, gtesmall())

prompt_template = """

### Instruction:
You are a helpful Educational Assistant who answers users' questions based on multiple contexts given to you.

Keep your answer short and to the point.

The evidence is the context of the pdf extract with metadata.

Carefully focus on the metadata, especially 'filename' and 'page' whenever answering.

Make sure to add filename and page number at the end of the sentence you are citing. If there is no page number or metadata available, you can ignore them.

Reply "Not applicable" if the text is irrelevant.

## Research:
{context}

## Question:
{question}

"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=db.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={'prompt': PROMPT}
)


def conversation_chat(user_input, history, generated, past):
    question = user_input  # Set user input as the question
    result = qa_chain({"query": question})

    # Check if "result" is present in the result dictionary
    if "result" in result:
        answer = result["result"]
    else:
        print(f"Error: {result.get('error_message', 'Unknown error')}")
        print("Full Response:", result)  # Print the full response for debugging
        answer = "Sorry, I couldn't find an answer."

    history.append((question, answer))
    past.append(user_input)  # Append user's input to the history
    generated.append(answer)  # Update the generated list with the current answer

    # Check if the answer contains a valid response or if it's a generated question
    if "ask me anything" not in answer.lower():
        return answer
    else:
        print("Generated question detected. Please ask a specific question.")
        return "Not applicable"


def initialize_session_state():
    history = []
    generated = []
    past = ["Hey! ðŸ‘‹"]
    return history, generated, past


def display_chat_history(history, generated, past):
    for i in range(len(generated)):
        if i < len(past):  # Check if i is within the range of past
            print(f"User: {past[i]}")
        print(f"Bot: {generated[i]}")


# Initialize session state
history, generated, past = initialize_session_state()

# Display chat history based on user input
while True:
    user_input = input("Question: Ask me anything (type 'exit' to quit): ")

    if user_input.lower() == 'exit':
        break

    # Display chat history before processing the user's input
    display_chat_history(history, generated, past)

    # Run LangChain model only if the user doesn't exit
    if user_input.lower() != 'exit':
        output = conversation_chat(user_input, history, generated, past)
        generated.append(output)
