from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.llms import Ollama
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from gtesmall2 import gtesmall

# Set the correct path to the "PDFs" folder in your Google Drive
pdfs_folder_path = '/home/navin/rag/PDFs'

# Load model directly
llm = Ollama(model="phi", callbacks=[StreamingStdOutCallbackHandler()])

# Initialize the directory loader with the correct path
raw_documents = DirectoryLoader(pdfs_folder_path,
                                glob="**/*.pdf",
                                loader_cls=PyPDFLoader,
                                show_progress=True,
                                use_multithreading=True).load()

# Verify that raw_documents is not empty
assert raw_documents, "No documents found in the specified folder."

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
documents = text_splitter.split_documents(raw_documents)

# Verify that documents is not empty
assert documents, "No documents were split into chunks."

# Load the embeddings into Chroma
print("Loading documents into Chroma\n")
db = Chroma.from_documents(documents, gtesmall())

prompt_template = """

### Instruction:
You are a helpful Educational Assistant who answers users' questions based on multiple contexts given to you.

Keep your answer short and to the point.

The evidence is the context of the pdf extract with metadata.

Carefully focus on the metadata, especially 'filename' and 'page' whenever answering.

Make sure to add filename and page number at the end of the sentence you are citing. If there is no page number or metadata available, you can ignore them.

Reply "Not applicable" if the text is irrelevant.

## Research:
{context}

## Question:
{question}

"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=db.as_retriever(search_kwargs={"k": 3}),
    chain_type_kwargs={'prompt': PROMPT}
)


def conversation_chat(user_input, history):
    question = user_input  # Set user input as the question
    result = qa_chain({"query": question})

    # Check if "result" is present in the result dictionary
    if "result" in result:
        answer = result["result"]
    else:
        print(f"Error: {result.get('error_message', 'Unknown error')}")
        print("Full Response:", result)  # Print the full response for debugging
        answer = "Sorry, I couldn't find an answer."

    history.append((question, answer))
    return answer


def initialize_session_state():
    history = []
    generated = ["Hello! Ask me anything about ðŸ¤—"]
    past = ["Hey! ðŸ‘‹"]
    return history, generated, past


def display_chat_history(history, generated, past):
    for i in range(len(generated)):
        print(f"User: {past[i]}")
        print(f"Bot: {generated[i]}")


# Initialize session state
history, generated, past = initialize_session_state()

# Display chat history based on user input
while True:
    user_input = input("Question: Ask me anything (type 'exit' to quit): ")

    if user_input.lower() == 'exit':
        break

    past.append(user_input)  # Append user's input to the history

    # Display chat history
    display_chat_history(history, generated, past)

    # Run LangChain model only if user doesn't exit
    if user_input.lower() != 'exit':
        output = conversation_chat(user_input, history)
        generated.append(output)
